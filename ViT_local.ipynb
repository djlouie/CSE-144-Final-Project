{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK0ORq8CMqxN"
      },
      "outputs": [],
      "source": [
        "# util imports\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# pytorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpz_vYLLMzg9",
        "outputId": "da183afe-c874-4a42-9788-094326eb8220"
      },
      "outputs": [],
      "source": [
        "# set seed\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# define device\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# path to train data\n",
        "train_path = os.path.join('unzipped_folder', 'train', 'train')\n",
        "print(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCotNS0XM2Gf",
        "outputId": "fd9427eb-1718-4c34-863f-21ede94a594f"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # Define the path to your ZIP file in Google Drive\n",
        "# zip_path = os.path.join(\"drive\", \"My Drive\", \"CSE-144-Final-Dataset\", \"ucsc-cse-144-winter-2025-final-project.zip\")\n",
        "\n",
        "# # Extract the ZIP file\n",
        "# extract_path = \"/content/unzipped_folder\"\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# print(f\"ZIP file extracted to: {extract_path}\")\n",
        "\n",
        "# # List extracted files\n",
        "# print(os.listdir(extract_path))\n",
        "\n",
        "train_path = \"./data/train/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCNSiRaiNs5a"
      },
      "outputs": [],
      "source": [
        "# composite the transforms\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "\n",
        "#     # this should theoretically be wtvr the pretrained model was trained on\n",
        "#     transforms.Normalize(mean=(0.48, 0.48, 0.48), std=(0.0039, 0.0039, 0.0039))\n",
        "# ])\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomRotation(20),\n",
        "    # transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), shear=10),\n",
        "    # transforms.RandomPerspective(distortion_scale=0.1, p=0.2, interpolation=2),  # Perspective transform\n",
        "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.15, hue=0.075),\n",
        "    # transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.RandAugment(num_ops=3, magnitude=6),  # RandAugment\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=(0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "\n",
        "trainset = datasets.ImageFolder(root=train_path, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "3sZU4GmJOr24",
        "outputId": "9ac83988-4ff8-42ce-8225-76ea4e5bef87"
      },
      "outputs": [],
      "source": [
        "# For matrix operations\n",
        "import numpy as np\n",
        "\n",
        "# Data visualizaton.\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "import random as rn\n",
        "\n",
        "fig, ax = plt.subplots(2, 6)\n",
        "fig.set_size_inches(6, 6)\n",
        "\n",
        "for i in range(2):\n",
        "    for j in range(6):\n",
        "        l=rn.randint(0, len(trainset))\n",
        "        ax[i, j].imshow(np.transpose(trainset[l][0].numpy(), (1, 2, 0)), cmap='gray')\n",
        "        ax[i, j].set_title('Label: ' + str(trainset[l][1]))\n",
        "        # Hide grid lines\n",
        "        ax[i, j].grid(False)\n",
        "        # Hide axes ticks\n",
        "        ax[i, j].set_xticks([])\n",
        "        ax[i, j].set_yticks([])\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np5w0Yd2PEAW",
        "outputId": "7bb9faf5-396a-4ff5-b160-ced8e130aa98"
      },
      "outputs": [],
      "source": [
        "# put the trainset into a dataloader\n",
        "train_loader = DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "class ViT_B16_Head(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ViT_B16_Head, self).__init__()\n",
        "        self.base_model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        # encoder_layer = nn.TransformerEncoderLayer(\n",
        "        #     d_model=768,  # Embedding dimension\n",
        "        #     nhead=12,     # Number of attention heads\n",
        "        #     dim_feedforward=3072,  # Hidden layer size in FFN\n",
        "        #     dropout=0.1,  # Dropout rate\n",
        "        #     activation='gelu'  # Activation function used in FFN\n",
        "        # )\n",
        "    \n",
        "        # self.base_model.encoder.layers.extend([encoder_layer, encoder_layer])\n",
        "\n",
        "        self.base_model.heads = nn.Sequential(\n",
        "            nn.Dropout(0.3),                      # Reduced dropout to retain more information\n",
        "            nn.Linear(768, 2048),                  \n",
        "            nn.BatchNorm1d(2048),                  # Normalization for stable training\n",
        "            nn.LeakyReLU(),                         \n",
        "            nn.Linear(2048, num_classes)           # Output layer (no softmax for training)\n",
        "        )\n",
        "        \n",
        "        for param in self.base_model.heads.parameters():\n",
        "            param.requires_grad = True\n",
        "            \n",
        "        # for name, param in self.base_model.named_parameters():\n",
        "        #     # if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
        "        #     if \"encoder.layer.11\" in name:\n",
        "        #         param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.base_model(x)\n",
        "        return outputs\n",
        "\n",
        "model = ViT_B16_Head()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J95K786PJgE"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW([\n",
        "        {'params': [param for param in model.base_model.parameters() if param.requires_grad is False], 'lr': 1e-6, 'weight_decay': 1e-6},  # Very small learning rate\n",
        "        {'params': [param for param in model.base_model.heads.parameters() if param.requires_grad is True], 'lr': 1e-4, 'weight_decay': 1e-3},  # Larger learning rate\n",
        "        # {'params': [param for name, param in model.base_model.named_parameters() if 'encoder.layer.10' in name or 'encoder.layer.11' in name], 'lr': 1e-5}  # Fine-tuned layers\n",
        "        # {'params': [param for name, param in model.base_model.named_parameters() if 'encoder.layer.11' in name], 'lr': 1e-4}  # Fine-tuned layers\n",
        "])\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training the network\n",
        "num_epochs = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-xzB_Y-PsRW"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(loader, model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            predicted = outputs.data.argmax(dim=1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total if total > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6X6_p8NP8br"
      },
      "outputs": [],
      "source": [
        "# save loss and accuracies\n",
        "train_losses = []\n",
        "train_acc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azX4URU7Q1E9",
        "outputId": "d762e71f-9d92-4312-a116-bf17305a00be"
      },
      "outputs": [],
      "source": [
        "images, target = next(iter(train_loader))\n",
        "images.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "TIlu5fqsPPUr",
        "outputId": "b1d11aed-afb2-409a-bcbe-8fa0b57cc1df"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for i, (images, targets) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        preds = model(images)\n",
        "        \n",
        "        # Calculate the loss\n",
        "        loss = loss_fn(preds, targets)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # calculate running loss\n",
        "        loss_value = loss.item()\n",
        "        running_loss += loss_value\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracy = calculate_accuracy(train_loader, model)\n",
        "    train_acc.append(train_accuracy)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.7f}, Train Acc: {train_accuracy:.7f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rzvCN6CQX6V"
      },
      "outputs": [],
      "source": [
        "# Plotting the training and validation loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "# plt.plot(test_losses, label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting the training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_acc, label='Train Accuracy')\n",
        "# plt.plot(test_acc, label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy9jlEtcR_JD"
      },
      "outputs": [],
      "source": [
        "def save_model(model, folder=\"models\", base_filename=\"ViT_model.pth\"):\n",
        "    os.makedirs(folder, exist_ok=True)  # Create folder if it doesn't exist\n",
        "    filepath = os.path.join(folder, base_filename)\n",
        "\n",
        "    # Check if file exists and iterate\n",
        "    if os.path.exists(filepath):\n",
        "        i = 1\n",
        "        filename, ext = os.path.splitext(base_filename)\n",
        "        while os.path.exists(os.path.join(folder, f\"{filename}_{i}{ext}\")):\n",
        "            i += 1\n",
        "        filepath = os.path.join(folder, f\"{filename}_{i}{ext}\")\n",
        "\n",
        "    torch.save(model.state_dict(), filepath)\n",
        "    print(f\"Model saved at: {filepath}\")\n",
        "    return filepath\n",
        "\n",
        "save_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Z2G70fXNGf"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6zpooYVXy4m"
      },
      "outputs": [],
      "source": [
        "# def testset_predictions(model, root_dir, transform):\n",
        "\n",
        "#     model.eval()\n",
        "\n",
        "#     preds = []\n",
        "\n",
        "#     # Get length of testset\n",
        "#     num_images = len(os.listdir(root_dir))\n",
        "\n",
        "#     for idx in range(num_images):\n",
        "#         # get the image path\n",
        "#         img_path = os.path.join(root_dir, f'{idx}.jpg')\n",
        "\n",
        "#         # open the image with the pillow library\n",
        "#         image = Image.open(img_path)\n",
        "\n",
        "#         # Transform if necessary\n",
        "#         if transform:\n",
        "#             image = transform(image)\n",
        "\n",
        "#         # add a dimension for batch\n",
        "#         image = torch.unsqueeze(image, 0)\n",
        "\n",
        "#         output = model(image.to(device))\n",
        "\n",
        "#         _, predicted_idx = torch.max(output.cpu().data, 1)\n",
        "\n",
        "#         preds.append([f'{idx}.jpg', predicted_idx.item()])\n",
        "\n",
        "#     # Once all predictions are collected, create the DataFrame\n",
        "#     df_preds = pd.DataFrame(preds, columns=['ID', 'Predicted_Label'])\n",
        "\n",
        "#     return df_preds\n",
        "\n",
        "def testset_predictions(model, root_dir, transform):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    # Get sorted list of image filenames\n",
        "    image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.jpg')])\n",
        "\n",
        "    with torch.no_grad():  # Ensure inference is done without gradient tracking\n",
        "        for img_name in image_files:\n",
        "            img_path = os.path.join(root_dir, img_name)\n",
        "\n",
        "            # Open image and ensure it has 3 channels (RGB)\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            # Apply transform if provided\n",
        "            if transform:\n",
        "                image = transform(image)\n",
        "            else:\n",
        "                image = transforms.ToTensor()(image)  # Convert to tensor if no transform\n",
        "\n",
        "            # Add batch dimension\n",
        "            image = image.unsqueeze(0).to(device)\n",
        "\n",
        "            # Get model prediction\n",
        "            output = model(image)\n",
        "            _, predicted_idx = torch.max(output.cpu().data, 1)\n",
        "\n",
        "            preds.append([img_name, predicted_idx.item()])\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df_preds = pd.DataFrame(preds, columns=['ID', 'Predicted_Label'])\n",
        "    \n",
        "    return df_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5TSezT7aM3a"
      },
      "outputs": [],
      "source": [
        "test_path = \"./data/test/\"\n",
        "\n",
        "# composite the transforms\n",
        "# test_transform = transforms.Compose([\n",
        "#     transforms.Grayscale(num_output_channels=3),\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "\n",
        "#     # this should theoretically be wtvr the pretrained model was trained on\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=(0.229, 0.224, 0.225))\n",
        "# ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    # transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),                \n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Ensure same normalization as pretraining\n",
        "])\n",
        "\n",
        "\n",
        "df_preds = testset_predictions(model, test_path, test_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhJWb7htap7J"
      },
      "outputs": [],
      "source": [
        "df_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHmr93LMb1Bc"
      },
      "outputs": [],
      "source": [
        "df_preds.to_csv('ViT_test_submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko_8K1mzdpvy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
